\documentclass{sig-alternate}

\usepackage{graphicx}


\usepackage{url}
\usepackage{hyperref} 
\hypersetup{breaklinks} 

\begin{document}
\title{Adaptive and Heterogeneous Hadoop MapReduce}
\author{Aaron Burkhart, Ross Nordstrom\\
        University of Colorado - Colorado Springs\\
        1420 Austin Bluffs Pkwy,\\
        Colorado Springs, CO 80918\\
        \texttt{\{aburkhar,rnordstr\}@uccs.edu}
       }
\date{April 2014}

\maketitle

\begin{abstract}
Hadoop is a commonly used Apache implementation of MapReduce, a technique for
parallel processing in distributed systems. One weakness of Hadoop is it does
not compute input splits or assign tasks based on the computers’ computational capabilities. By scaling
how much work is sent to a given computer based on its capabilities, we can
redce the difference in task completion time across the system, and thus improve
the overall performance of Hadoop. We call such a system – with computers of
varying capabilities – a heterogeneous environment. This research aims to develop
an input splitting and task assignment algorithm for Hadoop MapReduce that is sensitive 
to the hardware configurations of the nodes in the cluster.
\end{abstract}

\category{C.2.4}{Performance}{Cloud computing}
\terms{Performance, Design}
\keywords{Hadoop, MapReduce, Heterogeneous, Configuration, Cloud}

\section{Introduction}
Hadoop MapReduce is well-liked for its ability to dispatch tasks across workers
in a datacenter. Hypothetically, if the work is split up into equal parts and dispatched
efficiently across similar nodes, the system will work well. In reality, datacenters today are being
updated continuously with new hardware, while old hardware lingers in the datacente
. Some customers are willing to pay more for high performing nodes, while others prioritize cost over
performance. This diversity in customer needs contributes to diversity in datacenter hardware. 
Additionally, most consumers of datacenters are not the owners. Take Amazon EC2
or Windows Azure, for example. Companies maintain datacenters and sell access to
it as a service. 

Because modern datacenters are so frequently consumed as a service, the consumer
has little control over what hardware to which they will have access. Even if the
consumer had full control over the which hardware they accessed, it is costly and
inefficient to maintain a datacenter full of machines with identical hardware. 
Additionally, creating an algorithm to deploy a given MapReduce job across only 
identical machines would be difficult.

Rather than fight the structure of datacenters, adapting Hadoop to the environment
in which it is deployed would allow for optimal MapReduce performance in arbitrary
environments. This is especially important when deploying Hadoop using an IaaS 
(infrastructure as a service).

Our research aims to modify Hadoop v1.2.1 to adaptively vary the sizes of the input
splits and dispatch MapReduce tasks based on both the workers’ computational power
and scheduling status. In this paper, we discuss the existing implementation of Hadoop
and elaborate the issues it has when executing on heterogeneous environments. We then
describe our solution in more detail and evaluate our resulting implementation. Finally
we suggest directions for future and improvements to our progress.

Example reference\ref{section:motivation}.

\input{motivation}
\input{relatedwork}
\input{proposeddesign}
\input{evaluation}
\input{futurework}
\input{conclusion}

\bibliography{citations}{}
\bibliographystyle{plain}

\end{document}

